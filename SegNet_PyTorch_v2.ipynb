{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic segmentation of aerial images with deep networks\n",
    "\n",
    "This notebook presents a straightforward PyTorch implementation of a Fully Convolutional Network for semantic segmentation of aerial images. More specifically, we aim to automatically perform scene interpretation of images taken from a plane or a satellite by classifying every pixel into several land cover classes.\n",
    "\n",
    "As a demonstration, we are going to use the [SegNet architecture](http://mi.eng.cam.ac.uk/projects/segnet/) to segment aerial images over the cities of Vaihingen and Potsdam. The images are from the [ISPRS 2D Semantic Labeling dataset](http://www2.isprs.org/commissions/comm3/wg4/results.html). We will train a network to segment roads, buildings, vegetation and cars.\n",
    "\n",
    "This work is a PyTorch implementation of the baseline presented in [\"Beyond RGB: Very High Resolution Urban Remote Sensing With Multimodal Deep Networks \"](https://hal.archives-ouvertes.fr/hal-01636145), *Nicolas Audebert*, *Bertrand Le Saux* and *Sébastien Lefèvre*, ISPRS Journal, 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "This notebook requires a few useful libraries, e.g. `torch`, `scikit-image`, `numpy` and `matplotlib`. You can install everything using `pip install -r requirements.txt`.\n",
    "\n",
    "This is expected to run on GPU, and therefore you should use `torch` in combination with CUDA/cuDNN. This can probably be made to run on CPU but be warned that:\n",
    "  * you have to remove all calls to `torch.Tensor.cuda()` throughout this notebook,\n",
    "  * this will be very slow.\n",
    "  \n",
    "A \"small\" GPU should be enough, e.g. this runs fine on a 4.7GB Tesla K20m. It uses quite a lot of RAM as the dataset is stored in-memory (about 5GB for Vaihingen). You can spare some memory by disabling the caching below. 4GB should be more than enough without caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and stuff\n",
    "import numpy as np\n",
    "\n",
    "from skimage import io\n",
    "from glob import glob\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from encoding import lib\n",
    "\n",
    "import random\n",
    "import itertools\n",
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler\n",
    "import torch.nn.init\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2019 NVIDIA Corporation\r\n",
      "Built on Fri_Feb__8_19:08:17_PST_2019\r\n",
      "Cuda compilation tools, release 10.1, V10.1.105\r\n"
     ]
    }
   ],
   "source": [
    "# print(torch.__version__)\n",
    "# print(torch.nn.__file__)\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1'\n",
    "os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "os.environ['MASTER_PORT'] = '29500'\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "print(os.environ['CUDA_VISIBLE_DEVICES'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "There are several parameters than can be tuned to use this notebook with different datasets. The default parameters are suitable for the ISPRS dataset, but you can change them to work with your data.\n",
    "\n",
    "### Examples\n",
    "\n",
    "  * Binary classification: `N_CLASSES = 2`\n",
    "  * Multi-spectral data (e.g. IRRGB): `IN_CHANNELS = 4`\n",
    "  * New folder naming convention : `DATA_FOLDER = MAIN_FOLDER + 'sentinel2/sentinel2_img_{}.tif'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "WINDOW_SIZE = (256, 256) # Patch size\n",
    "STRIDE = 32 # Stride for testing\n",
    "IN_CHANNELS = 3 # Number of input channels (e.g. RGB)\n",
    "FOLDER = \"../ISPRS_DATASET/\" # Replace with your \"/path/to/the/ISPRS/dataset/folder/\"\n",
    "BATCH_SIZE = 2 # Number of samples in a mini-batch\n",
    "\n",
    "LABELS = [\"roads\", \"buildings\", \"low veg.\", \"trees\", \"cars\", \"clutter\"] # Label names\n",
    "N_CLASSES = len(LABELS) # Number of classes\n",
    "WEIGHTS = torch.ones(N_CLASSES) # Weights for class balancing\n",
    "CACHE = True # Store the dataset in-memory\n",
    "\n",
    "DATASET = 'Vaihingen'\n",
    "\n",
    "if DATASET == 'Potsdam':\n",
    "    MAIN_FOLDER = FOLDER + 'Potsdam/'\n",
    "    DATA_FOLDER = MAIN_FOLDER + '3_Ortho_IRRG/top_potsdam_{}_{}_IRRG.tif'\n",
    "    LABEL_FOLDER = MAIN_FOLDER + '5_Labels_for_participants/top_potsdam_{}_{}_label.tif'\n",
    "    ERODED_FOLDER = MAIN_FOLDER + '5_Labels_for_participants_no_Boundary/top_potsdam_{}_label_noBoundary.tif'    \n",
    "elif DATASET == 'Vaihingen':\n",
    "    MAIN_FOLDER = FOLDER + 'Vaihingen/'\n",
    "    DATA_FOLDER = MAIN_FOLDER + 'top/top_mosaic_09cm_area{}.tif'\n",
    "    LABEL_FOLDER = MAIN_FOLDER + 'gts_for_participants/top_mosaic_09cm_area{}.tif'\n",
    "#     ERODED_FOLDER = MAIN_FOLDER + 'gts_eroded_for_participants/top_mosaic_09cm_area{}_noBoundary.tif' # original line\n",
    "    ERODED_FOLDER = MAIN_FOLDER + 'gts_eroded_complete/top_mosaic_09cm_area{}.tif'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the dataset\n",
    "\n",
    "First, let's check that we are able to access the dataset and see what's going on. We use ```scikit-image``` for image manipulation.\n",
    "\n",
    "As the ISPRS dataset is stored with a ground truth in the RGB format, we need to define the color palette that can map the label id to its RGB color. We define two helper functions to convert from numeric to colors and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISPRS color palette\n",
    "# Let's define the standard ISPRS color palette\n",
    "palette = {0 : (255, 255, 255), # Impervious surfaces (white)\n",
    "           1 : (0, 0, 255),     # Buildings (blue)\n",
    "           2 : (0, 255, 255),   # Low vegetation (cyan)\n",
    "           3 : (0, 255, 0),     # Trees (green)\n",
    "           4 : (255, 255, 0),   # Cars (yellow)\n",
    "           5 : (255, 0, 0),     # Clutter (red)\n",
    "           6 : (0, 0, 0)}       # Undefined (black)\n",
    "\n",
    "invert_palette = {v: k for k, v in palette.items()}\n",
    "\n",
    "def convert_to_color(arr_2d, palette=palette):\n",
    "    \"\"\" Numeric labels to RGB-color encoding \"\"\"\n",
    "    arr_3d = np.zeros((arr_2d.shape[0], arr_2d.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "    for c, i in palette.items():\n",
    "        m = arr_2d == c\n",
    "        arr_3d[m] = i\n",
    "\n",
    "    return arr_3d\n",
    "\n",
    "def convert_from_color(arr_3d, palette=invert_palette):\n",
    "    \"\"\" RGB-color encoding to grayscale labels \"\"\"\n",
    "    arr_2d = np.zeros((arr_3d.shape[0], arr_3d.shape[1]), dtype=np.uint8)\n",
    "\n",
    "    for c, i in palette.items():\n",
    "        m = np.all(arr_3d == np.array(c).reshape(1, 1, 3), axis=2)\n",
    "        arr_2d[m] = i\n",
    "\n",
    "    return arr_2d\n",
    "\n",
    "# We load one tile from the dataset and we display it\n",
    "img = io.imread(DATA_FOLDER.format(2,10))\n",
    "fig = plt.figure()\n",
    "fig.add_subplot(121)\n",
    "plt.imshow(img)\n",
    "\n",
    "# We load the ground truth\n",
    "gt = io.imread(LABEL_FOLDER.format(2,10))\n",
    "fig.add_subplot(122)\n",
    "plt.imshow(gt)\n",
    "plt.show()\n",
    "\n",
    "# We also check that we can convert the ground truth into an array format\n",
    "array_gt = convert_from_color(gt)\n",
    "print(\"Ground truth in numerical format has shape ({},{}) : \\n\".format(*array_gt.shape[:2]), array_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls ./ISPRS_DATASET/Potsdam/5_Labels_for_participants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define a bunch of utils functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "\n",
    "def get_random_pos(img, window_shape):\n",
    "    \"\"\" Extract of 2D random patch of shape window_shape in the image \"\"\"\n",
    "    w, h = window_shape\n",
    "    W, H = img.shape[-2:]\n",
    "    x1 = random.randint(0, W - w - 1)\n",
    "    x2 = x1 + w\n",
    "    y1 = random.randint(0, H - h - 1)\n",
    "    y2 = y1 + h\n",
    "    return x1, x2, y1, y2\n",
    "\n",
    "def CrossEntropy2d(input, target, weight=None, size_average=True):\n",
    "    \"\"\" 2D version of the cross entropy loss \"\"\"\n",
    "    dim = input.dim()\n",
    "    if dim == 2:\n",
    "        return F.cross_entropy(input, target, weight, size_average)\n",
    "    elif dim == 4:\n",
    "        output = input.view(input.size(0),input.size(1), -1)\n",
    "        output = torch.transpose(output,1,2).contiguous()\n",
    "        output = output.view(-1,output.size(2))\n",
    "        target = target.view(-1)\n",
    "        return F.cross_entropy(output, target,weight, size_average)\n",
    "    else:\n",
    "        raise ValueError('Expected 2 or 4 dimensions (got {})'.format(dim))\n",
    "\n",
    "def accuracy(input, target):\n",
    "    return 100 * float(np.count_nonzero(input == target)) / target.size\n",
    "\n",
    "def sliding_window(top, step=10, window_size=(20,20)):\n",
    "    \"\"\" Slide a window_shape window across the image with a stride of step \"\"\"\n",
    "    for x in range(0, top.shape[0], step):\n",
    "        if x + window_size[0] > top.shape[0]:\n",
    "            x = top.shape[0] - window_size[0]\n",
    "        for y in range(0, top.shape[1], step):\n",
    "            if y + window_size[1] > top.shape[1]:\n",
    "                y = top.shape[1] - window_size[1]\n",
    "            yield x, y, window_size[0], window_size[1]\n",
    "            \n",
    "def count_sliding_window(top, step=10, window_size=(20,20)):\n",
    "    \"\"\" Count the number of windows in an image \"\"\"\n",
    "    c = 0\n",
    "    for x in range(0, top.shape[0], step):\n",
    "        if x + window_size[0] > top.shape[0]:\n",
    "            x = top.shape[0] - window_size[0]\n",
    "        for y in range(0, top.shape[1], step):\n",
    "            if y + window_size[1] > top.shape[1]:\n",
    "                y = top.shape[1] - window_size[1]\n",
    "            c += 1\n",
    "    return c\n",
    "\n",
    "def grouper(n, iterable):\n",
    "    \"\"\" Browse an iterator by chunk of n elements \"\"\"\n",
    "    it = iter(iterable)\n",
    "    while True:\n",
    "        chunk = tuple(itertools.islice(it, n))\n",
    "        if not chunk:\n",
    "            return\n",
    "        yield chunk\n",
    "\n",
    "def metrics(predictions, gts, label_values=LABELS):\n",
    "    cm = confusion_matrix(\n",
    "            gts,\n",
    "            predictions,\n",
    "            range(len(label_values)))\n",
    "    \n",
    "#     print(\"Confusion matrix :\")\n",
    "#     print(cm)\n",
    "    \n",
    "#     print(\"---\")\n",
    "    \n",
    "    # Compute global accuracy\n",
    "    total = sum(sum(cm))\n",
    "    accuracy = sum([cm[x][x] for x in range(len(cm))])\n",
    "    accuracy *= 100 / float(total)\n",
    "#     print(\"{} pixels processed\".format(total))\n",
    "#     print(\"Total accuracy : {}%\".format(accuracy))\n",
    "    \n",
    "#     print(\"---\")\n",
    "    \n",
    "    # Compute F1 score\n",
    "    F1Score = np.zeros(len(label_values))\n",
    "    for i in range(len(label_values)):\n",
    "        try:\n",
    "            F1Score[i] = 2. * cm[i,i] / (np.sum(cm[i,:]) + np.sum(cm[:,i]))\n",
    "        except:\n",
    "            # Ignore exception if there is no element in class i for test set\n",
    "            pass\n",
    "#     print(\"F1Score :\")\n",
    "    for l_id, score in enumerate(F1Score):\n",
    "#         print(\"{}: {}\".format(label_values[l_id], score))\n",
    "        pass\n",
    "#     print(\"---\")\n",
    "        \n",
    "    # Compute kappa coefficient\n",
    "    total = np.sum(cm)\n",
    "    pa = np.trace(cm) / float(total)\n",
    "    pe = np.sum(np.sum(cm, axis=0) * np.sum(cm, axis=1)) / float(total*total)\n",
    "    kappa = (pa - pe) / (1 - pe);\n",
    "#     print(\"Kappa: \" + str(kappa))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "\n",
    "We define a PyTorch dataset (```torch.utils.data.Dataset```) that loads all the tiles in memory and performs random sampling. Tiles are stored in memory on the fly.\n",
    "\n",
    "The dataset also performs random data augmentation (horizontal and vertical flips) and normalizes the data in [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "\n",
    "class ISPRS_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, ids, data_files=DATA_FOLDER, label_files=LABEL_FOLDER,\n",
    "                            cache=False, augmentation=True):\n",
    "        super(ISPRS_dataset, self).__init__()\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        self.cache = cache\n",
    "        \n",
    "        # List of files\n",
    "        self.data_files = [DATA_FOLDER.format(id) for id in ids]\n",
    "        self.label_files = [LABEL_FOLDER.format(id) for id in ids]\n",
    "\n",
    "        # Sanity check : raise an error if some files do not exist\n",
    "        for f in self.data_files + self.label_files:\n",
    "            if not os.path.isfile(f):\n",
    "                raise KeyError('{} is not a file !'.format(f))\n",
    "        \n",
    "        # Initialize cache dicts\n",
    "        self.data_cache_ = {}\n",
    "        self.label_cache_ = {}\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        # Default epoch size is 10 000 samples\n",
    "        return 10000\n",
    "    \n",
    "    @classmethod\n",
    "    def data_augmentation(cls, *arrays, flip=True, mirror=True):\n",
    "        will_flip, will_mirror = False, False\n",
    "        if flip and random.random() < 0.5:\n",
    "            will_flip = True\n",
    "        if mirror and random.random() < 0.5:\n",
    "            will_mirror = True\n",
    "        \n",
    "        results = []\n",
    "        for array in arrays:\n",
    "            if will_flip:\n",
    "                if len(array.shape) == 2:\n",
    "                    array = array[::-1, :]\n",
    "                else:\n",
    "                    array = array[:, ::-1, :]\n",
    "            if will_mirror:\n",
    "                if len(array.shape) == 2:\n",
    "                    array = array[:, ::-1]\n",
    "                else:\n",
    "                    array = array[:, :, ::-1]\n",
    "            results.append(np.copy(array))\n",
    "            \n",
    "        return tuple(results)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        # Pick a random image\n",
    "        random_idx = random.randint(0, len(self.data_files) - 1)\n",
    "        \n",
    "        # If the tile hasn't been loaded yet, put in cache\n",
    "        if random_idx in self.data_cache_.keys():\n",
    "            data = self.data_cache_[random_idx]\n",
    "        else:\n",
    "            # Data is normalized in [0, 1]\n",
    "            data = 1/255 * np.asarray(io.imread(self.data_files[random_idx]).transpose((2,0,1)), dtype='float32')\n",
    "            if self.cache:\n",
    "                self.data_cache_[random_idx] = data\n",
    "            \n",
    "        if random_idx in self.label_cache_.keys():\n",
    "            label = self.label_cache_[random_idx]\n",
    "        else: \n",
    "            # Labels are converted from RGB to their numeric values\n",
    "            label = np.asarray(convert_from_color(io.imread(self.label_files[random_idx])), dtype='int64')\n",
    "            if self.cache:\n",
    "                self.label_cache_[random_idx] = label\n",
    "\n",
    "        # Get a random patch\n",
    "        x1, x2, y1, y2 = get_random_pos(data, WINDOW_SIZE)\n",
    "        data_p = data[:, x1:x2,y1:y2]\n",
    "        label_p = label[x1:x2,y1:y2]\n",
    "        \n",
    "        # Data augmentation\n",
    "        data_p, label_p = self.data_augmentation(data_p, label_p)\n",
    "\n",
    "        # Return the torch.Tensor values\n",
    "        return (torch.from_numpy(data_p),\n",
    "                torch.from_numpy(label_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "\n",
    "class ISPRS_val(torch.utils.data.Dataset):\n",
    "    def __init__(self, ids,length, data_files=DATA_FOLDER, label_files=LABEL_FOLDER, augmentation=True):\n",
    "        super(ISPRS_val, self).__init__()\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        \n",
    "        self.length = length\n",
    "        # List of files\n",
    "        self.data_files = [DATA_FOLDER.format(id) for id in ids]\n",
    "        self.label_files = [LABEL_FOLDER.format(id) for id in ids]\n",
    "\n",
    "        # Sanity check : raise an error if some files do not exist\n",
    "        for f in self.data_files + self.label_files:\n",
    "            if not os.path.isfile(f):\n",
    "                raise KeyError('{} is not a file !'.format(f))\n",
    "        # Initialize cache dicts\n",
    "        self.data_cache_ = {}\n",
    "        self.label_cache_ = {}\n",
    "     \n",
    " \n",
    "    def __len__(self):\n",
    "        # Default epoch size is 10 000 samples\n",
    "        return self.length\n",
    "    \n",
    "    @classmethod\n",
    "    def data_augmentation(cls, *arrays, flip=True, mirror=True):\n",
    "        will_flip, will_mirror = False, False\n",
    "        if flip and random.random() < 0.5:\n",
    "            will_flip = True\n",
    "        if mirror and random.random() < 0.5:\n",
    "            will_mirror = True\n",
    "        \n",
    "        results = []\n",
    "        for array in arrays:\n",
    "            if will_flip:\n",
    "                if len(array.shape) == 2:\n",
    "                    array = array[::-1, :]\n",
    "                else:\n",
    "                    array = array[:, ::-1, :]\n",
    "            if will_mirror:\n",
    "                if len(array.shape) == 2:\n",
    "                    array = array[:, ::-1]\n",
    "                else:\n",
    "                    array = array[:, :, ::-1]\n",
    "            results.append(np.copy(array))\n",
    "            \n",
    "        return tuple(results)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        # Pick a random image\n",
    "        random_idx = random.randint(0, len(self.data_files) - 1)\n",
    "        \n",
    "        # If the tile hasn't been loaded yet, put in cache\n",
    "        if random_idx in self.data_cache_.keys():\n",
    "            data = self.data_cache_[random_idx]\n",
    "        else:\n",
    "            # Data is normalized in [0, 1]\n",
    "            data = 1/255 * np.asarray(io.imread(self.data_files[random_idx]).transpose((2,0,1)), dtype='float32')\n",
    " \n",
    "        if random_idx in self.label_cache_.keys():\n",
    "            label = self.label_cache_[random_idx]\n",
    "        else: \n",
    "            # Labels are converted from RGB to their numeric values\n",
    "            label = np.asarray(convert_from_color(io.imread(self.label_files[random_idx])), dtype='int64')\n",
    "           \n",
    "        # Get a random patch\n",
    "        x1, x2, y1, y2 = get_random_pos(data, WINDOW_SIZE)\n",
    "        data_p = data[:, x1:x2,y1:y2]\n",
    "        label_p = label[x1:x2,y1:y2]\n",
    "\n",
    "        \n",
    "        # Data augmentation\n",
    "        data_p, label_p = self.data_augmentation(data_p, label_p)\n",
    "\n",
    "        # Return the torch.Tensor values\n",
    "        return (torch.from_numpy(data_p),\n",
    "                torch.from_numpy(label_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network definition\n",
    "\n",
    "We can now define the Fully Convolutional network based on the SegNet architecture. We could use any other network as drop-in replacement, provided that the output has dimensions `(N_CLASSES, W, H)` where `W` and `H` are the sliding window dimensions (i.e. the network should preserve the spatial dimensions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Parameter\n",
    "\n",
    "# Building the context encoding module\n",
    "\n",
    "\n",
    "class Encoding(Module):\n",
    "    def __init__(self, D, K):\n",
    "        super(Encoding, self).__init__()\n",
    "        # init codewords and smoothing factor\n",
    "        self.D, self.K = D, K\n",
    "        self.codewords = Parameter(torch.Tensor(K, D), requires_grad=True)\n",
    "        self.scale = Parameter(torch.Tensor(K), requires_grad=True)\n",
    "        self.reset_params()\n",
    "\n",
    "    def reset_params(self):\n",
    "        std1 = 1./((self.K*self.D)**(1/2))\n",
    "        self.codewords.data.uniform_(-std1, std1)\n",
    "        self.scale.data.uniform_(-1, 0)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # input X is a 4D tensor\n",
    "        assert(X.size(1) == self.D)\n",
    "        B, D = X.size(0), self.D\n",
    "        if X.dim() == 3:\n",
    "            # BxDxN => BxNxD\n",
    "            X = X.transpose(1, 2).contiguous()\n",
    "        elif X.dim() == 4:\n",
    "            # BxDxHxW => Bx(HW)xD\n",
    "            X = X.view(B, D, -1).transpose(1, 2).contiguous()\n",
    "        else:\n",
    "            raise RuntimeError('Encoding Layer unknown input dims!')\n",
    "        # assignment weights BxNxK\n",
    "        A = F.softmax(scaled_l2(X, self.codewords, self.scale), dim=2)\n",
    "\n",
    "        # aggregate\n",
    "        E = aggregate(A, X, self.codewords)\n",
    "        return E\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "            + 'N x ' + str(self.D) + '=>' + str(self.K) + 'x' \\\n",
    "            + str(self.D) + ')'\n",
    "    \n",
    "    \n",
    "class EncModule(nn.Module):\n",
    "    def __init__(self, in_channels, nclass, ncodes=32, se_loss=True, norm_layer=None):\n",
    "        super(EncModule, self).__init__()\n",
    "        self.se_loss = se_loss\n",
    "        self.encoding = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, 1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Encoding(D=in_channels, K=ncodes),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Mean(dim=1))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels),\n",
    "            nn.Sigmoid())\n",
    "        if self.se_loss:\n",
    "            self.selayer = nn.Linear(in_channels, nclass)\n",
    "\n",
    "    def forward(self, x):\n",
    "        en = self.encoding(x)\n",
    "        b, c, _, _ = x.size()\n",
    "        gamma = self.fc(en)\n",
    "        y = gamma.view(b, c, 1, 1)\n",
    "        outputs = [F.relu_(x + x * y)]\n",
    "        if self.se_loss:\n",
    "            outputs.append(self.selayer(en))\n",
    "        return tuple(outputs)\n",
    "\n",
    "    \n",
    "class EncHead(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, se_loss=True, lateral=True,\n",
    "                 norm_layer=None, up_kwargs=None):\n",
    "        super(EncHead, self).__init__()\n",
    "        self.se_loss = se_loss\n",
    "        self.lateral = lateral\n",
    "        self.up_kwargs = up_kwargs\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 512, 3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True))\n",
    "        if lateral:\n",
    "            self.connect = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(512, 512, kernel_size=1, bias=False),\n",
    "                    nn.ReLU(inplace=True)),\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(1024, 512, kernel_size=1, bias=False),\n",
    "                    nn.ReLU(inplace=True)),\n",
    "            ])\n",
    "            self.fusion = nn.Sequential(\n",
    "                    nn.Conv2d(3*512, 512, kernel_size=3, padding=1, bias=False),\n",
    "                    nn.ReLU(inplace=True))\n",
    "        self.encmodule = EncModule(512, out_channels, ncodes=32,\n",
    "            se_loss=se_loss, norm_layer=norm_layer)\n",
    "        self.conv6 = nn.Sequential(nn.Dropout2d(0.1, False),\n",
    "                                   nn.Conv2d(512, out_channels, 1))\n",
    "\n",
    "    def forward(self, *inputs):\n",
    "        feat = self.conv5(inputs[-1])\n",
    "        if self.lateral:\n",
    "            \n",
    "            c2 = self.connect[0](inputs[1])\n",
    "            c3 = self.connect[1](inputs[2])\n",
    "            \n",
    "            feat = self.fusion(torch.cat([feat, c2, c3], 1))    \n",
    "        outs = list(self.encmodule(feat))\n",
    "        outs[0] = self.conv6(outs[0])\n",
    "        return tuple(outs)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable, Function\n",
    "from torch.autograd.function import once_differentiable\n",
    "\n",
    "\n",
    "try:\n",
    "    from queue import Queue\n",
    "except ImportError:\n",
    "    from Queue import Queue\n",
    "\n",
    "class Mean(nn.Module):\n",
    "    def __init__(self, dim, keep_dim=False):\n",
    "        super(Mean, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.keep_dim = keep_dim\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.mean(self.dim, self.keep_dim)\n",
    "    \n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class _scaled_l2(Function):\n",
    "#     @staticmethod\n",
    "#     def forward(ctx, X, C, S):\n",
    "#         if X.is_cuda:\n",
    "#             SL = lib.gpu.scaled_l2_forward(X, C, S)\n",
    "#         else:\n",
    "#             SL = lib.cpu.scaled_l2_forward(X, C, S)\n",
    "#         ctx.save_for_backward(X, C, S, SL)\n",
    "#         return SL\n",
    "\n",
    "#     @staticmethod\n",
    "#     def backward(ctx, gradSL):\n",
    "#         X, C, S, SL = ctx.saved_variables\n",
    "#         if X.is_cuda:\n",
    "#             gradX, gradC, gradS = lib.gpu.scaled_l2_backward(gradSL, X, C, S, SL)\n",
    "#         else:\n",
    "#             gradX, gradC, gradS = lib.cpu.scaled_l2_backward(gradSL, X, C, S, SL)\n",
    "#         return gradX, gradC, gradS\n",
    "\n",
    "# def scaled_l2(X, C, S):\n",
    "#     r\"\"\" scaled_l2 distance\n",
    "\n",
    "#     .. math::\n",
    "#         sl_{ik} = s_k \\|x_i-c_k\\|^2\n",
    "\n",
    "#     Shape:\n",
    "#         - Input: :math:`X\\in\\mathcal{R}^{B\\times N\\times D}`\n",
    "#           :math:`C\\in\\mathcal{R}^{K\\times D}` :math:`S\\in \\mathcal{R}^K`\n",
    "#           (where :math:`B` is batch, :math:`N` is total number of features,\n",
    "#           :math:`K` is number is codewords, :math:`D` is feature dimensions.)\n",
    "#         - Output: :math:`E\\in\\mathcal{R}^{B\\times N\\times K}`\n",
    "#     \"\"\"\n",
    "#     return _scaled_l2.apply(X, C, S)\n",
    "\n",
    "# class _aggregate(Function):\n",
    "#     @staticmethod\n",
    "#     def forward(ctx, A, X, C):\n",
    "#         # A \\in(BxNxK) R \\in(BxNxKxD) => E \\in(BxNxD)\n",
    "#         ctx.save_for_backward(A, X, C)\n",
    "#         if A.is_cuda:\n",
    "#             E = lib.gpu.aggregate_forward(A, X, C)\n",
    "#         else:\n",
    "#             E = lib.cpu.aggregate_forward(A, X, C)\n",
    "#         return E\n",
    "\n",
    "#     @staticmethod\n",
    "#     def backward(ctx, gradE):\n",
    "#         A, X, C = ctx.saved_variables\n",
    "#         if A.is_cuda:\n",
    "#             gradA, gradX, gradC = lib.gpu.aggregate_backward(gradE, A, X, C)\n",
    "#         else:\n",
    "#             gradA, gradX, gradC = lib.cpu.aggregate_backward(gradE, A, X, C)\n",
    "#         return gradA, gradX, gradC\n",
    "\n",
    "# def aggregate(A, X, C):\n",
    "#     r\"\"\" Aggregate operation, aggregate the residuals of inputs (:math:`X`) with repect\n",
    "#     to the codewords (:math:`C`) with assignment weights (:math:`A`).\n",
    "\n",
    "#     .. math::\n",
    "\n",
    "#         e_{k} = \\sum_{i=1}^{N} a_{ik} (x_i - d_k)\n",
    "\n",
    "#     Shape:\n",
    "#         - Input: :math:`A\\in\\mathcal{R}^{B\\times N\\times K}`\n",
    "#           :math:`X\\in\\mathcal{R}^{B\\times N\\times D}` :math:`C\\in\\mathcal{R}^{K\\times D}`\n",
    "#           (where :math:`B` is batch, :math:`N` is total number of features,\n",
    "#           :math:`K` is number is codewords, :math:`D` is feature dimensions.)\n",
    "#         - Output: :math:`E\\in\\mathcal{R}^{B\\times K\\times D}`\n",
    "\n",
    "#     Examples:\n",
    "#         >>> B,N,K,D = 2,3,4,5\n",
    "#         >>> A = Variable(torch.cuda.DoubleTensor(B,N,K).uniform_(-0.5,0.5), requires_grad=True)\n",
    "#         >>> X = Variable(torch.cuda.DoubleTensor(B,N,D).uniform_(-0.5,0.5), requires_grad=True)\n",
    "#         >>> C = Variable(torch.cuda.DoubleTensor(K,D).uniform_(-0.5,0.5), requires_grad=True)\n",
    "#         >>> func = encoding.aggregate()\n",
    "#         >>> E = func(A, X, C)\n",
    "#     \"\"\"\n",
    "#     return _aggregate.apply(A, X, C)\n",
    "\n",
    "\n",
    "def scaled_l2(X, C, S):\n",
    "    \"\"\"\n",
    "    scaled_l2 distance\n",
    "    Args:\n",
    "        X (b*n*d):  original feature input\n",
    "        C (k*d):    code words, with k codes, each with d dimension\n",
    "        S (k):      scale cofficient\n",
    "    Return:\n",
    "        D (b*n*k):  relative distance to each code\n",
    "    Note:\n",
    "        apparently the X^2 + C^2 - 2XC computation is 2x faster than\n",
    "        elementwise sum, perhaps due to friendly cache in gpu\n",
    "    \"\"\"\n",
    "    assert X.shape[-1] == C.shape[-1], \"input, codeword feature dim mismatch\"\n",
    "    assert S.numel() == C.shape[0], \"scale, codeword num mismatch\"\n",
    "    \"\"\"\n",
    "    # simplier but slower\n",
    "    X = X.unsqueeze(2)\n",
    "    C = C[None, None,...]\n",
    "    norm = torch.norm(X-C, dim=-1).pow(2.0)\n",
    "    scaled_norm = S * norm\n",
    "    \"\"\"\n",
    "    b, n, d = X.shape\n",
    "    X = X.view(-1, d)  # [bn, d]\n",
    "    Ct = C.t()  # [d, k]\n",
    "    D = X.mm(Ct) # [bn, k]\n",
    "    D = D.view(b, n, -1) # [b, n, k]\n",
    "#     X2 = X.pow(2.0).sum(-1, keepdim=True)  # [bn, 1]\n",
    "#     C2 = Ct.pow(2.0).sum(0, keepdim=True)  # [1, k]\n",
    "#     norm = X2 + C2 - 2.0 * X.mm(Ct)  # [bn, k]\n",
    "#     scaled_norm = S * norm\n",
    "#     D = scaled_norm.view(b, n, -1)  # [b, n, k]\n",
    "    return D\n",
    "\n",
    "def aggregate(A, X, C):\n",
    "    \"\"\"\n",
    "    aggregate residuals from N samples\n",
    "    Args:\n",
    "        A (b*n*k):  weight of each feature contribute to code residual\n",
    "        X (b*n*d):  original feature input\n",
    "        C (k*d):    code words, with k codes, each with d dimension\n",
    "    Return:\n",
    "        E (b*k*d):  residuals to each code\n",
    "    \"\"\"\n",
    "    assert X.shape[-1] == C.shape[-1], \"input, codeword feature dim mismatch\"\n",
    "    assert A.shape[:2] == X.shape[:2], \"weight, input dim mismatch\"\n",
    "    b, n, d = X.shape\n",
    "#     X = X.unsqueeze(2)  # [b, n, d] -> [b, n, 1, d]\n",
    "#     C = C[None, None, ...]  # [k, d] -> [1, 1, k, d]\n",
    "#     A = A.unsqueeze(-1)  # [b, n, k] -> [b, n, k, 1]\n",
    "#     R = (X - C) * A  # [b, n, k, d]\n",
    "#     E = R.sum(dim=1)  # [b, k, d]\n",
    "\n",
    "    X = X.sum(dim=1).view(b,d) #[b, n, d] -> [b,d]\n",
    "    X = X.unsqueeze(1) #[ b, 1, d]\n",
    "    C = C[None, ...]  # [k, d] -> [1, k, d]\n",
    "    E = X + C\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SegNet(nn.Module):\n",
    "    # SegNet network\n",
    "    @staticmethod\n",
    "    def weight_init(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.kaiming_normal(m.weight.data)\n",
    "    \n",
    "    def __init__(self, in_channels=IN_CHANNELS, out_channels=N_CLASSES):\n",
    "        super(SegNet, self).__init__()\n",
    "        self.pool = nn.MaxPool2d(2, return_indices=True)\n",
    "        self.unpool = nn.MaxUnpool2d(2)\n",
    "        \n",
    "        self.conv1_1 = nn.Conv2d(in_channels, 64, 3, padding=1)\n",
    "        self.conv1_1_bn = nn.BatchNorm2d(64)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.conv1_2_bn = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv2_1 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv2_1_bn = nn.BatchNorm2d(128)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.conv2_2_bn = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv3_1 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.conv3_1_bn = nn.BatchNorm2d(256)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.conv3_2_bn = nn.BatchNorm2d(256)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.conv3_3_bn = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv4_1 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.conv4_1_bn = nn.BatchNorm2d(512)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv4_2_bn = nn.BatchNorm2d(512)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv4_3_bn = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv5_1 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv5_1_bn = nn.BatchNorm2d(512)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv5_2_bn = nn.BatchNorm2d(512)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv5_3_bn = nn.BatchNorm2d(512)\n",
    "\n",
    "        self.conv5_3_D = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv5_3_D_bn = nn.BatchNorm2d(512)\n",
    "        self.conv5_2_D = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv5_2_D_bn = nn.BatchNorm2d(512)\n",
    "        self.conv5_1_D = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv5_1_D_bn = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv4_3_D = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv4_3_D_bn = nn.BatchNorm2d(512)\n",
    "        self.conv4_2_D = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv4_2_D_bn = nn.BatchNorm2d(512)\n",
    "        self.conv4_1_D = nn.Conv2d(512, 256, 3, padding=1)\n",
    "        self.conv4_1_D_bn = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv3_3_D = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.conv3_3_D_bn = nn.BatchNorm2d(256)\n",
    "        self.conv3_2_D = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.conv3_2_D_bn = nn.BatchNorm2d(256)\n",
    "        self.conv3_1_D = nn.Conv2d(256, 128, 3, padding=1)\n",
    "        self.conv3_1_D_bn = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv2_2_D = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.conv2_2_D_bn = nn.BatchNorm2d(128)\n",
    "        self.conv2_1_D = nn.Conv2d(128, 64, 3, padding=1)\n",
    "        self.conv2_1_D_bn = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv1_2_D = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.conv1_2_D_bn = nn.BatchNorm2d(64)\n",
    "        self.conv1_1_D = nn.Conv2d(64, out_channels, 3, padding=1)\n",
    "        \n",
    "        #may set norm_layer to Sync Batch Norm as in munanet later\n",
    "        self.context_encoder = EncHead(out_channels, out_channels, se_loss=True, norm_layer=None,\n",
    "                    lateral=False, up_kwargs=None)\n",
    "        \n",
    "        self.apply(self.weight_init)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder block 1\n",
    "        x = self.conv1_1_bn(F.relu(self.conv1_1(x)))\n",
    "        x = self.conv1_2_bn(F.relu(self.conv1_2(x)))\n",
    "        x, mask1 = self.pool(x)\n",
    "        \n",
    "        # Encoder block 2\n",
    "        x = self.conv2_1_bn(F.relu(self.conv2_1(x)))\n",
    "        x = self.conv2_2_bn(F.relu(self.conv2_2(x)))\n",
    "        x, mask2 = self.pool(x)\n",
    "        \n",
    "        # Encoder block 3\n",
    "        x = self.conv3_1_bn(F.relu(self.conv3_1(x)))\n",
    "        x = self.conv3_2_bn(F.relu(self.conv3_2(x)))\n",
    "        x = self.conv3_3_bn(F.relu(self.conv3_3(x)))\n",
    "        x, mask3 = self.pool(x)\n",
    "        \n",
    "        # Encoder block 4\n",
    "        x = self.conv4_1_bn(F.relu(self.conv4_1(x)))\n",
    "        x = self.conv4_2_bn(F.relu(self.conv4_2(x)))\n",
    "        x = self.conv4_3_bn(F.relu(self.conv4_3(x)))\n",
    "        x, mask4 = self.pool(x)\n",
    "        \n",
    "        # Encoder block 5\n",
    "        x = self.conv5_1_bn(F.relu(self.conv5_1(x)))\n",
    "        x = self.conv5_2_bn(F.relu(self.conv5_2(x)))\n",
    "        x = self.conv5_3_bn(F.relu(self.conv5_3(x)))\n",
    "        x, mask5 = self.pool(x)\n",
    "        \n",
    "        # Decoder block 5\n",
    "        x = self.unpool(x, mask5)\n",
    "        x = self.conv5_3_D_bn(F.relu(self.conv5_3_D(x)))\n",
    "        x = self.conv5_2_D_bn(F.relu(self.conv5_2_D(x)))\n",
    "        x = self.conv5_1_D_bn(F.relu(self.conv5_1_D(x)))\n",
    "        \n",
    "        # Decoder block 4\n",
    "        x = self.unpool(x, mask4)\n",
    "        x = self.conv4_3_D_bn(F.relu(self.conv4_3_D(x)))\n",
    "        x = self.conv4_2_D_bn(F.relu(self.conv4_2_D(x)))\n",
    "        x = self.conv4_1_D_bn(F.relu(self.conv4_1_D(x)))\n",
    "        \n",
    "        # Decoder block 3\n",
    "        x = self.unpool(x, mask3)\n",
    "        x = self.conv3_3_D_bn(F.relu(self.conv3_3_D(x)))\n",
    "        x = self.conv3_2_D_bn(F.relu(self.conv3_2_D(x)))\n",
    "        x = self.conv3_1_D_bn(F.relu(self.conv3_1_D(x)))\n",
    "        \n",
    "        # Decoder block 2\n",
    "        x = self.unpool(x, mask2)\n",
    "        x = self.conv2_2_D_bn(F.relu(self.conv2_2_D(x)))\n",
    "        x = self.conv2_1_D_bn(F.relu(self.conv2_1_D(x)))\n",
    "        \n",
    "        # Decoder block 1\n",
    "        x = self.unpool(x, mask1)\n",
    "        x = self.conv1_2_D_bn(F.relu(self.conv1_2_D(x)))\n",
    "#         x = F.log_softmax(self.conv1_1_D(x)) #这里使用log_softmax+NLLLoss\n",
    "#         print('lx', x.size())\n",
    "        x = self.conv1_1_D(x)\n",
    "        x = list(self.context_encoder(x))[0]\n",
    "        x = F.log_softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SegNet_Loss(nn.Module):\n",
    "#     def __init__(self, model, loss):\n",
    "#         super(SegNet_Loss, self).__init__()\n",
    "#         self.model = model\n",
    "#         self.loss = loss\n",
    "\n",
    "#     def forward(self, inputs, targets):\n",
    "#         outputs = self.model(inputs)\n",
    "#         loss = self.loss(outputs, targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now instantiate the network using the specified parameters. By default, the weights will be initialized using the [He policy](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the network\n",
    "net = SegNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We download and load the pre-trained weights from VGG-16 on ImageNet. This step is optional but it makes the network converge faster. We skip the weights from VGG-16 that have no counterpart in SegNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    from urllib.request import URLopener\n",
    "except ImportError:\n",
    "    from urllib import URLopener\n",
    "\n",
    "# Download VGG-16 weights from PyTorch\n",
    "vgg_url = 'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth'\n",
    "if not os.path.isfile('./vgg16_bn-6c64b313.pth'):\n",
    "    weights = URLopener().retrieve(vgg_url, './vgg16_bn-6c64b313.pth')\n",
    "\n",
    "vgg16_weights = torch.load('./vgg16_bn-6c64b313.pth')\n",
    "mapped_weights = {}\n",
    "for k_vgg, k_segnet in zip(vgg16_weights.keys(), net.state_dict().keys()):\n",
    "    if \"features\" in k_vgg:\n",
    "        mapped_weights[k_segnet] = vgg16_weights[k_vgg]\n",
    "        print(\"Mapping {} to {}\".format(k_vgg, k_segnet))\n",
    "        \n",
    "try:\n",
    "    net.load_state_dict(mapped_weights)\n",
    "    print(\"Loaded VGG-16 weights in SegNet !\")\n",
    "except:\n",
    "    # Ignore missing keys\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we load the network on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.cuda()\n",
    "\n",
    "net = torch.nn.DataParallel(net, device_ids=[0, 1]).cuda()\n",
    "\n",
    "\n",
    "\n",
    "# torch.distributed.init_process_group(backend='nccl')\n",
    "# net.cuda()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     net = nn.parallel.DistributedDataParallel(net, device_ids=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "We now create a train/test split. If you want to use another dataset, you have to adjust the method to collect all filenames. In our case, we specify a fixed train/test split for the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load the datasets\n",
    "if DATASET == 'Potsdam':\n",
    "    all_files = sorted(glob(LABEL_FOLDER.replace('{}', '*')))\n",
    "    all_ids = [\"_\".join(f.split('_')[3:5]) for f in all_files]\n",
    "elif DATASET == 'Vaihingen':\n",
    "    #all_ids = \n",
    "    all_files = sorted(glob(LABEL_FOLDER.replace('{}', '*')))\n",
    "    all_ids = [f.split('area')[-1].split('.')[0] for f in all_files]\n",
    "# Random tile numbers for train/test split\n",
    "train_ids = random.sample(all_ids, 2 * len(all_ids) // 3 + 1)\n",
    "test_ids = list(set(all_ids) - set(train_ids))\n",
    "\n",
    "# Exemple of a train/test split on Vaihingen :\n",
    "# train_ids = ['1', '3', '23', '26', '7', '11', '13', '28', '17', '32', '34', '37']\n",
    "# test_ids = ['5', '21', '15', '30']\n",
    "\n",
    "train_ids = ['1', '3', '23', '26', '7', '11', '13', '28', '17', '32', '34', '37', '5', '21', '15', '30']\n",
    "test_ids = list(set(all_ids) - set(train_ids))\n",
    "\n",
    "print(\"Tiles for training : \", train_ids)\n",
    "print(\"Tiles for testing : \", test_ids)\n",
    "\n",
    "train_set = ISPRS_dataset(train_ids, cache=CACHE)\n",
    "train_loader = torch.utils.data.DataLoader(train_set,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[0][0].size(), train_set[0][1].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing the optimizer\n",
    "\n",
    "We use the standard Stochastic Gradient Descent algorithm to optimize the network's weights.\n",
    "\n",
    "The encoder is trained at half the learning rate of the decoder, as we rely on the pre-trained VGG-16 weights. We use the ``torch.optim.lr_scheduler`` to reduce the learning rate by 10 after 25, 35 and 45 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_lr = 0.0001\n",
    "params_dict = dict(net.named_parameters())\n",
    "params = []\n",
    "for key, value in params_dict.items():\n",
    "    if '_D' in key:\n",
    "        # Decoder weights are trained at the nominal learning rate\n",
    "        params += [{'params':[value],'lr': base_lr}]\n",
    "    else:\n",
    "        # Encoder weights are trained at lr / 2 (we have VGG-16 weights as initialization)\n",
    "        params += [{'params':[value],'lr': base_lr / 2}]\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=base_lr, momentum=0.9, weight_decay=0.0005)\n",
    "# We define the scheduler\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [25, 35, 45], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempted Val accuracy - currently not working\n",
    "\n",
    "\n",
    "# val_ids = test_ids[:1]\n",
    "# val_len = len(val_ids)\n",
    "# val_set = ISPRS_val(val_ids, val_len)\n",
    "# val_loader = torch.utils.data.DataLoader(val_set,batch_size=BATCH_SIZE)\n",
    "\n",
    "# print(len(val_set))\n",
    "# print(\"validated with\", val_ids)\n",
    "\n",
    "# def val_acc(net, val_ids):\n",
    "#     all_acc = np.zeros(10000)\n",
    "\n",
    "#     # Switch the network to inference mode\n",
    "#     net.eval()\n",
    "\n",
    "# #     for img, gt, gt_e in tqdm(zip(test_images, test_labels, eroded_labels), total=len(test_ids), leave=False): #original line\n",
    "#     for batch_idx, (data, target) in enumerate(val_loader):\n",
    "#         data, target = data.cuda(), target.cuda()\n",
    "#         output = net(data)\n",
    "        \n",
    "#         pred = np.argmax(output.data.cpu().numpy()[0], axis=0)\n",
    "#         gt = target.data.cpu().numpy()[0]\n",
    "\n",
    "#         all_acc[batch_idx] = accuracy(pred, gt)\n",
    "        \n",
    "# #         clear_output()\n",
    "#         rgb = np.asarray(255 * np.transpose(data.data.cpu().numpy()[0],(1,2,0)), dtype='uint8')\n",
    "\n",
    "#         plt.savefig('fintune_loss.png')\n",
    "#         fig = plt.figure()\n",
    "#         fig.add_subplot(131)\n",
    "#         plt.imshow(rgb)\n",
    "#         plt.title('RGB')\n",
    "#         fig.add_subplot(132)\n",
    "#         plt.imshow(convert_to_color(gt))\n",
    "#         plt.title('Ground truth')\n",
    "#         fig.add_subplot(133)\n",
    "#         plt.title('Prediction')\n",
    "#         plt.imshow(convert_to_color(pred))\n",
    "#         plt.show()\n",
    "        \n",
    "        \n",
    "#     net.train()\n",
    "# #     print(\"RUN 1X-->>>\", all_acc)\n",
    "#     return np.asarray(all_acc).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "#Added these\n",
    "\n",
    "def train(net, optimizer, epochs, scheduler=None, weights=WEIGHTS, save_epoch = 5):\n",
    "    losses = np.zeros(1000000)\n",
    "    mean_losses = np.zeros(100000000)\n",
    "    weights = weights.cuda()\n",
    "    \n",
    "    acc = np.zeros(1000000)\n",
    "    mean_acc = np.zeros(1000000)\n",
    "\n",
    "    criterion = nn.NLLLoss2d(weight=weights)\n",
    "    iter_ = 0\n",
    "    \n",
    "    for e in range(1, epochs + 1):\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        net.train()\n",
    "        batch_acc = []\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "#             print(\"CHECK\")\n",
    "\n",
    "#             replaced the below lines with everything up to loss.backward\n",
    "            output = net(data)\n",
    "            loss = CrossEntropy2d(output, target, weight=weights)\n",
    "#             loss = net(data, target)\n",
    "# #             print(loss.shape)\n",
    "#             loss = loss.sum()\n",
    "            \n",
    "            \n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "#             losses[iter_] = loss.data[0] #original version\n",
    "            losses[iter_] = loss.item()\n",
    "            mean_losses[iter_] = np.mean(losses[max(0,iter_-100):iter_])\n",
    "            pred = np.argmax(output.data.cpu().numpy()[0], axis=0)\n",
    "            gt = target.data.cpu().numpy()[0]\n",
    "            \n",
    "            acc[iter_] = 100 * float(np.count_nonzero(pred == gt)) / gt.size\n",
    "            mean_acc[iter_] = np.mean(acc[max(0,iter_-100):iter_])\n",
    "\n",
    "#             print(\"SHAPES\", output.shape, target.shape)\n",
    "            if iter_ % 100 == 0:\n",
    "#                 pass\n",
    "                clear_output()       \n",
    "#                 acc[iter_] = val_acc(net, val_ids)\n",
    "#                 print(\"acc[iter_]\", acc[iter_])\n",
    "\n",
    "                rgb = np.asarray(255 * np.transpose(data.data.cpu().numpy()[0],(1,2,0)), dtype='uint8')\n",
    "                \n",
    "# #                 print('Train (epoch {}/{}) [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAccuracy: {}'.format(\n",
    "# #                     e, epochs, batch_idx, len(train_loader),\n",
    "# #                     100. * batch_idx / len(train_loader), loss.data[0], accuracy(pred, gt)))\n",
    "                print('Train (epoch {}/{}) [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAccuracy: {}'.format(\n",
    "                    e, epochs, batch_idx, len(train_loader),\n",
    "#                     100. * batch_idx / len(train_loader), loss.data, accuracy(pred, gt)))\n",
    "                    100. * batch_idx / len(train_loader), losses[iter_], acc[iter_] ))\n",
    "\n",
    "                plt.plot(mean_losses[:iter_]) and plt.show() \n",
    "                plt.plot(mean_acc[:iter_]) and plt.show() #I added\n",
    "\n",
    "                plt.savefig('fintune_loss.png')\n",
    "                fig = plt.figure()\n",
    "                fig.add_subplot(131)\n",
    "                plt.imshow(rgb)\n",
    "                plt.title('RGB')\n",
    "                fig.add_subplot(132)\n",
    "                plt.imshow(convert_to_color(gt))\n",
    "                plt.title('Ground truth')\n",
    "                fig.add_subplot(133)\n",
    "                plt.title('Prediction')\n",
    "                plt.imshow(convert_to_color(pred))\n",
    "                plt.show()\n",
    "                \n",
    "  \n",
    "            iter_ += 1\n",
    "            \n",
    "           \n",
    "            del(data, target, loss)    \n",
    "                \n",
    "        if e % save_epoch == 0:\n",
    "            # We validate with the largest possible stride for faster computing\n",
    "#             acc = test(net, test_ids, all=False, stride=min(WINDOW_SIZE))\n",
    "#             torch.save(net.state_dict(), './segnet256_epoch{}_{}'.format(e, acc))\n",
    "            torch.save(net.state_dict(), './segnet256_fintune_epoch{}/{}_{}'.format(e+35, epochs, accuracy(pred, gt) ))\n",
    "    torch.save(net.state_dict(), './segnet_fintune_final')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network\n",
    "\n",
    "Let's train the network for 50 epochs. The `matplotlib` graph is periodically udpated with the loss plot and a sample inference. Depending on your GPU, this might take from a few hours (Titan Pascal) to a full day (old K20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train(net, optimizer, 15, scheduler)\n",
    "train(net, optimizer, 200, scheduler) #using 400 to see where it stops before using that as stopping point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, filename='checkpoint.pth.tar'):\n",
    "    # Note: Input model & optimizer should be pre-defined.  This routine only updates their states.\n",
    "    start_epoch = 0\n",
    "    if os.path.isfile(filename):\n",
    "        print(\"=> loading checkpoint '{}'\".format(filename))\n",
    "        checkpoint = torch.load(filename)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "#         losslogger = checkpoint['losslogger']\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(filename, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(filename))\n",
    "\n",
    "    return model, optimizer, start_epoch#, losslogger\n",
    "\n",
    "# net, optimizer, start_epoch = load_checkpoint(net, optimizer, filename='./checkpoints_SegNet/segnet256_epoch35_88.73044107893148')\n",
    "# filename='./checkpoints_SegNet/segnet256_epoch35_88.73044107893148'\n",
    "# checkpoint = torch.load(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the network\n",
    "\n",
    "Now that the training has ended, we can load the final weights and test the network using a reasonable stride, e.g. half or a quarter of the window size. Inference time depends on the chosen stride, e.g. a step size of 32 (75% overlap) will take ~15 minutes, but no overlap will take only one minute or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, test_ids, all=False, stride=WINDOW_SIZE[0], batch_size=BATCH_SIZE, window_size=WINDOW_SIZE):\n",
    "    # Use the network on the test set\n",
    "    test_images = (1 / 255 * np.asarray(io.imread(DATA_FOLDER.format(id)), dtype='float32') for id in test_ids)\n",
    "    test_labels = (np.asarray(io.imread(LABEL_FOLDER.format(id)), dtype='uint8') for id in test_ids)\n",
    "    eroded_labels = (convert_from_color(io.imread(ERODED_FOLDER.format(id))) for id in test_ids)\n",
    "    all_preds = []\n",
    "    all_gts = []\n",
    "    \n",
    "    # Switch the network to inference mode\n",
    "    net.eval()\n",
    "\n",
    "#     for img, gt, gt_e in tqdm(zip(test_images, test_labels, eroded_labels), total=len(test_ids), leave=False): #original line\n",
    "    for _, (img, gt, gt_e) in enumerate(zip(test_images, test_labels, eroded_labels)):\n",
    "\n",
    "        pred = np.zeros(img.shape[:2] + (N_CLASSES,))\n",
    "\n",
    "        total = count_sliding_window(img, step=stride, window_size=window_size) // batch_size\n",
    "#         for i, coords in enumerate(tqdm(grouper(batch_size, sliding_window(img, step=stride, window_size=window_size)), total=total, leave=False)):\n",
    "        for _, coords in enumerate(grouper(batch_size, sliding_window(img, step=stride, window_size=window_size))):\n",
    "  \n",
    "            # Display in progress results\n",
    "#             if i > 0 and total > 10 and i % int(10 * total / 100) == 0:\n",
    "#                     _pred = np.argmax(pred, axis=-1)\n",
    "#                     fig = plt.figure()\n",
    "#                     fig.add_subplot(1,3,1)\n",
    "#                     plt.imshow(np.asarray(255 * img, dtype='uint8'))\n",
    "#                     fig.add_subplot(1,3,2)\n",
    "#                     plt.imshow(convert_to_color(_pred))\n",
    "#                     fig.add_subplot(1,3,3)\n",
    "#                     plt.imshow(gt)\n",
    "#                     clear_output()\n",
    "#                     plt.show()\n",
    "                    \n",
    "            # Build the tensor\n",
    "            image_patches = [np.copy(img[x:x+w, y:y+h]).transpose((2,0,1)) for x,y,w,h in coords]\n",
    "            image_patches = np.asarray(image_patches)\n",
    "            image_patches = Variable(torch.from_numpy(image_patches).cuda(), volatile=True)\n",
    "            \n",
    "            # Do the inference\n",
    "            outs = net(image_patches)\n",
    "            outs = outs.data.cpu().numpy()\n",
    "            \n",
    "            # Fill in the results array\n",
    "            for out, (x, y, w, h) in zip(outs, coords):\n",
    "                out = out.transpose((1,2,0))\n",
    "                pred[x:x+w, y:y+h] += out\n",
    "            del(outs)\n",
    "\n",
    "        pred = np.argmax(pred, axis=-1)\n",
    "\n",
    "        # Display the result\n",
    "        clear_output()\n",
    "        fig = plt.figure()\n",
    "        fig.add_subplot(1,3,1)\n",
    "        plt.imshow(np.asarray(255 * img, dtype='uint8'))\n",
    "        fig.add_subplot(1,3,2)\n",
    "        plt.imshow(convert_to_color(pred))\n",
    "        fig.add_subplot(1,3,3)\n",
    "        plt.imshow(gt)\n",
    "        plt.show()\n",
    "\n",
    "        all_preds.append(pred)\n",
    "        all_gts.append(gt_e)\n",
    "\n",
    "        clear_output()\n",
    "        # Compute some metrics\n",
    "        metrics(pred.ravel(), gt_e.ravel())\n",
    "        accuracy = metrics(np.concatenate([p.ravel() for p in all_preds]), np.concatenate([p.ravel() for p in all_gts]).ravel())\n",
    "    if all:\n",
    "        return accuracy, all_preds, all_gts\n",
    "    else:\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load('./segnet256_fintune_epoch45_76.67083740234375'))\n",
    "acc, all_preds, all_gts = test(net, test_ids, all=True, stride=32)\n",
    "print('overall accuracy: %f' % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the results\n",
    "\n",
    "We can visualize and save the resulting tiles for qualitative assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p, id_ in zip(all_preds, test_ids):\n",
    "    img = convert_to_color(p)\n",
    "    plt.imshow(img) and plt.show()\n",
    "    io.imsave('./inference_tiles/inference_tile_{}.png'.format(id_), img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "if DATASET == 'Potsdam':\n",
    "    all_files = sorted(glob(LABEL_FOLDER.replace('{}', '*')))\n",
    "    all_ids = [\"_\".join(f.split('_')[3:5]) for f in all_files]\n",
    "elif DATASET == 'Vaihingen':\n",
    "    #all_ids = \n",
    "    all_files = sorted(glob(DATA_FOLDER.replace('{}', '*')))\n",
    "    all_ids = [f.split('area')[-1].split('.')[0] for f in all_files]\n",
    "print(len(all_ids), all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ids = ['1', '3', '23', '26', '7', '11', '13', '28', '17', '32', '34', '37', '5', '21', '15', '30']\n",
    "# test_ids = list(set(all_ids) - set(train_ids))\n",
    "\n",
    "print(\"Tiles for training : \", train_ids)\n",
    "print(\"Tiles for testing : \", test_ids)\n",
    "\n",
    "train_set = ISPRS_dataset(train_ids, cache=CACHE)\n",
    "train_loader = torch.utils.data.DataLoader(train_set,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(net, test_ids, all=False, stride=WINDOW_SIZE[0], batch_size=BATCH_SIZE, window_size=WINDOW_SIZE):\n",
    "    # Use the network on the test set\n",
    "    test_images = (1 / 255 * np.asarray(io.imread(DATA_FOLDER.format(id)), dtype='float32') for id in test_ids)\n",
    "    all_preds = []\n",
    "    all_gts = []\n",
    "    \n",
    "    # Switch the network to inference mode\n",
    "    net.eval()\n",
    "\n",
    "    for img, in tqdm(zip(test_images), total=len(test_ids), leave=False):\n",
    "        pred = np.zeros(img.shape[:2] + (N_CLASSES,))\n",
    "\n",
    "        total = count_sliding_window(img, step=stride, window_size=window_size) // batch_size\n",
    "        for i, coords in enumerate(tqdm(grouper(batch_size, sliding_window(img, step=stride, window_size=window_size)), total=total, leave=False)):\n",
    "            # Display in progress results\n",
    "            if i > 0 and total > 10 and i % int(10 * total / 100) == 0:\n",
    "                    _pred = np.argmax(pred, axis=-1)\n",
    "#                     fig = plt.figure()\n",
    "#                     fig.add_subplot(1,3,1)\n",
    "#                     plt.imshow(np.asarray(255 * img, dtype='uint8'))\n",
    "#                     fig.add_subplot(1,3,2)\n",
    "#                     plt.imshow(convert_to_color(_pred))\n",
    "#                     fig.add_subplot(1,3,3)\n",
    "#                     plt.imshow(gt)\n",
    "#                     clear_output()\n",
    "#                     plt.show()\n",
    "                    \n",
    "            # Build the tensor\n",
    "            image_patches = [np.copy(img[x:x+w, y:y+h]).transpose((2,0,1)) for x,y,w,h in coords]\n",
    "            image_patches = np.asarray(image_patches)\n",
    "            image_patches = Variable(torch.from_numpy(image_patches).cuda(), volatile=True)\n",
    "            \n",
    "            # Do the inference\n",
    "            outs = net(image_patches)\n",
    "            outs = outs.data.cpu().numpy()\n",
    "            \n",
    "            # Fill in the results array\n",
    "            for out, (x, y, w, h) in zip(outs, coords):\n",
    "                out = out.transpose((1,2,0))\n",
    "                pred[x:x+w, y:y+h] += out\n",
    "            del(outs)\n",
    "\n",
    "        pred = np.argmax(pred, axis=-1)\n",
    "\n",
    "        # Display the result\n",
    "        clear_output()\n",
    "        fig = plt.figure()\n",
    "        fig.add_subplot(1,3,1)\n",
    "        plt.imshow(np.asarray(255 * img, dtype='uint8'))\n",
    "        fig.add_subplot(1,3,2)\n",
    "        plt.imshow(convert_to_color(pred))\n",
    "        fig.add_subplot(1,3,3)\n",
    "        plt.imshow(gt)\n",
    "        plt.show()\n",
    "\n",
    "        all_preds.append(pred)\n",
    "        clear_output()\n",
    "    return all_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load('./segnet_fintune_final'))\n",
    "all_preds = inference(net, test_ids, all=True, stride=32)\n",
    "# print('overall accuracy: %f' % (acc))\n",
    "\n",
    "for p, id_ in zip(all_preds, test_ids):\n",
    "    img = convert_to_color(p)\n",
    "    plt.imshow(img) and plt.show()\n",
    "    io.imsave('./inference_SegNet/submission/top_mosaic_09cm_area{}_class.tif'.format(id_), img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls inference_SegNet/submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_preds),len(test_ids)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
